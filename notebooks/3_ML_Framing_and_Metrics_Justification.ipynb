{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c751be",
   "metadata": {},
   "source": [
    "## 3. ML Framing & Metrics Justification\n",
    "\n",
    "This section defines how the business problem is translated into a machine learning task, how success is measured, and which trade-offs guide model selection.  \n",
    "It acts as the *conceptual bridge* between the Product Requirements (Deliverable 1), the Data Understanding & EDA (Deliverable 2), and the downstream modeling and deployment decisions (Deliverables 4–8).\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Problem Type: Regression\n",
    "\n",
    "The core task is to **predict the test bench time of a car given its configuration**.  \n",
    "The target variable is a continuous, real-valued quantity measured in time units (e.g., minutes).\n",
    "\n",
    "From an ML framing perspective, this naturally maps to a **supervised regression problem**, not classification or ranking.\n",
    "\n",
    "Key characteristics justifying regression framing:\n",
    "\n",
    "- **Continuity of the outcome**  \n",
    "  Test duration varies smoothly with configuration changes (e.g., engine type, transmission, weight), rather than falling into discrete categories.\n",
    "\n",
    "- **Operational relevance of magnitude**  \n",
    "  The *absolute size* of the error matters: a 5-minute error has a very different operational impact than a 30-minute error.\n",
    "\n",
    "- **Downstream decision usage**  \n",
    "  Predictions are consumed by production planners to allocate test bench slots and sequence vehicles. This requires numeric estimates, not labels.\n",
    "\n",
    "Connection to other deliverables:\n",
    "- **Deliverable 1 (PRD):** Defines the prediction target explicitly as test time per vehicle.\n",
    "- **Deliverable 2 (EDA):** Confirms sufficient variance and signal in features to support regression feasibility.\n",
    "- **Deliverable 4 (Model Experiments):** Explores multiple regression families (linear, tree-based, ensemble).\n",
    "- **Deliverable 8 (Post-Launch Plan):** Assumes continuous monitoring of prediction error over time.\n",
    "\n",
    "Importantly, while more complex formulations (e.g., probabilistic regression or interval prediction) are conceivable, the initial framing prioritizes **point estimation** to keep the system interpretable and operationally simple in early deployment stages.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Evaluation Metrics: RMSE and MAE\n",
    "\n",
    "Metric selection follows an *outcome-oriented* logic: metrics are chosen not for mathematical elegance alone, but for their alignment with operational risk and planning costs.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is selected as the **primary evaluation metric**.\n",
    "\n",
    "Justification:\n",
    "- Penalizes larger errors more strongly due to squaring.\n",
    "- Aligns with the business risk of **severe mis-scheduling**, where large underestimates or overestimates can cascade into idle benches or bottlenecks.\n",
    "- Provides a single, continuous measure suitable for model comparison during experimentation.\n",
    "\n",
    "Operational interpretation:\n",
    "> A lower RMSE directly translates into fewer extreme scheduling failures.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "MAE is used as a **secondary, robustness-oriented metric**.\n",
    "\n",
    "Justification:\n",
    "- Less sensitive to outliers than RMSE.\n",
    "- Provides a stable estimate of *typical* prediction error.\n",
    "- Useful for diagnosing whether performance degradation is driven by rare extreme cases or by systematic bias.\n",
    "\n",
    "Why both metrics are needed:\n",
    "- RMSE captures *risk sensitivity*.\n",
    "- MAE captures *typical planner experience*.\n",
    "\n",
    "Connection to other deliverables:\n",
    "- **Deliverable 1 (Success Metrics):** RMSE explicitly defined as success criterion.\n",
    "- **Deliverable 5 (KPI–OKR Mapping):** RMSE and MAE serve as leading indicators for downstream KPIs such as idle bench reduction.\n",
    "- **Deliverable 6 (Risk & Failure Analysis):** Divergence between RMSE and MAE can signal outlier-driven failure modes.\n",
    "\n",
    "Metrics not chosen (and why):\n",
    "- R² alone is insufficient due to weak interpretability for operational planning.\n",
    "- MAPE is unstable near small denominators and less intuitive in this context.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Baseline Definition\n",
    "\n",
    "A baseline is essential to demonstrate that the ML system adds value beyond existing heuristics.\n",
    "\n",
    "Two baselines are defined:\n",
    "\n",
    "#### Baseline A: Global Mean Test Time\n",
    "- Predicts the historical average test duration for all cars.\n",
    "- Represents the *implicit status quo* used in manual planning.\n",
    "\n",
    "#### Baseline B: Simple Linear Regression\n",
    "- Uses a limited subset of key numerical and encoded categorical features.\n",
    "- Represents a transparent, low-complexity analytical model.\n",
    "\n",
    "Purpose of baselines:\n",
    "- Establish a **minimum performance threshold**.\n",
    "- Prevent overestimating the value of complex models.\n",
    "- Provide a reference for model risk discussions.\n",
    "\n",
    "Connection to other deliverables:\n",
    "- **Deliverable 2 (EDA):** Informs which features are reasonable even for a simple baseline.\n",
    "- **Deliverable 4 (Experiment Log):** Shows incremental value over baselines.\n",
    "- **Deliverable 3 (this section):** Frames baseline choice as a product decision, not a technical afterthought.\n",
    "\n",
    "A model that fails to outperform these baselines consistently is considered **non-viable for production**, regardless of theoretical appeal.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Trade-offs and Decision Rationale\n",
    "\n",
    "ML model selection is framed as a sequence of **explicit trade-offs**, not a search for maximum accuracy in isolation.\n",
    "\n",
    "#### Accuracy vs. Model Complexity\n",
    "- More complex models (e.g., Gradient Boosting) capture nonlinear interactions.\n",
    "- However, complexity increases:\n",
    "  - Maintenance cost\n",
    "  - Risk of overfitting\n",
    "  - Difficulty of explanation to stakeholders\n",
    "\n",
    "Decision principle:\n",
    "> Prefer the *simplest model* that meets accuracy and robustness requirements.\n",
    "\n",
    "#### Interpretability vs. Performance\n",
    "- Linear models offer high transparency but limited expressiveness.\n",
    "- Tree-based ensembles offer improved performance with partial interpretability (feature importance, SHAP).\n",
    "\n",
    "Given the Mercedes-Benz context:\n",
    "- Full black-box models are less desirable.\n",
    "- Partial explainability is sufficient if decision logic remains defensible.\n",
    "\n",
    "#### Latency vs. Throughput\n",
    "- Real-time dashboards require low-latency predictions.\n",
    "- Batch planning tolerates higher latency.\n",
    "\n",
    "Framing decision:\n",
    "- Models must support **near-real-time inference**.\n",
    "- Training cost is secondary to inference stability.\n",
    "\n",
    "Connection to other deliverables:\n",
    "- **Deliverable 4 (Model Log):** Documents how trade-offs influenced final model choice.\n",
    "- **Deliverable 6 (Risk Analysis):** Trade-offs are linked to failure anticipation.\n",
    "- **Deliverable 8 (Lifecycle Plan):** Complexity affects retraining and rollback strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Role within the ML Workflow\n",
    "\n",
    "ML Framing & Metrics Justification serves as the **decision anchor** of the entire workflow:\n",
    "\n",
    "- Translates business goals into ML objectives.\n",
    "- Constrains model experimentation space.\n",
    "- Ensures consistency between data analysis, modeling, evaluation, and deployment.\n",
    "- Enables transparent communication with non-ML stakeholders.\n",
    "\n",
    "Without this framing, later improvements in model performance risk becoming **locally optimal but globally misaligned** with product and operational goals.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The regression framing, metric choices, baselines, and trade-offs together ensure that:\n",
    "- Model performance is meaningful in operational terms.\n",
    "- Decisions are explainable and defensible.\n",
    "- The system aligns with Mercedes-Benz requirements for robustness, predictability, and lifecycle sustainability.\n",
    "\n",
    "This section thus operationalizes AI Product Thinking at the point where technical modeling decisions begin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a6b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deliverable 3: ML Framing & Metrics Justification ===\n",
      "\n",
      "1. Problem Type\n",
      "- Supervised regression\n",
      "- Continuous target: test bench duration (y)\n",
      "- Static, tabular dataset\n",
      "- No temporal forecasting\n",
      "\n",
      "2. Data Handling Policies\n",
      "\n",
      "2.1 Missingness Policy\n",
      "- No NaN values observed\n",
      "- Zeros represent structural absence of options\n",
      "- No imputation will be applied\n",
      "- Models must handle sparsity explicitly\n",
      "\n",
      "2.2 Outlier Policy\n",
      "- Extreme test durations are real operational cases\n",
      "- No row-level removal or clipping\n",
      "- Robustness handled via loss functions or regularization\n",
      "\n",
      "2.3 Rare Configuration Policy\n",
      "- Rare configurations are retained\n",
      "- No grouping or collapsing at data level\n",
      "- Generalization risk acknowledged and monitored\n",
      "\n",
      "3. Leakage Prevention Strategy\n",
      "- ID column must never be used as a feature\n",
      "- Repeated configurations exist (up to 9 occurrences)\n",
      "- Train/validation splits must be configuration-aware\n",
      "- Identical configurations must not leak across splits\n",
      "Approved validation strategies:\n",
      "- Group-based split using full configuration signature\n",
      "- OR time-aware split if temporal order becomes available\n",
      "\n",
      "4. Feature Policy\n",
      "- Nominal features (X0–X8):\n",
      "  - Treated as categorical\n",
      "  - Encoding strategy must preserve interpretability\n",
      "- Binary features (X10+):\n",
      "  - Sparse, high-dimensional\n",
      "  - No feature removal without justification\n",
      "  - Regularization preferred over hard selection\n",
      "\n",
      "5. Model Eligibility Constraints\n",
      "Eligible model families must:\n",
      "- Support regression\n",
      "- Handle high-dimensional sparse inputs\n",
      "- Be stable under rare configurations\n",
      "\n",
      "Interpretability constraint:\n",
      "- Model behavior must be explainable at feature-group level\n",
      "- Pure black-box models require additional justification\n",
      "\n",
      "6. Evaluation Metrics\n",
      "Primary metric:\n",
      "- RMSE (penalizes large scheduling errors)\n",
      "\n",
      "Secondary metrics:\n",
      "- MAE (robust to outliers)\n",
      "- Error distribution diagnostics\n",
      "\n",
      "Metric usage constraints:\n",
      "- Metrics must be reported with confidence intervals\n",
      "- Single-point estimates are insufficient\n",
      "\n",
      "7. Baseline Definition\n",
      "Mandatory baselines for Deliverable 4:\n",
      "- Global mean predictor\n",
      "- Simple linear regression\n",
      "Purpose of baselines:\n",
      "- Sanity check\n",
      "- Value justification of complex models\n",
      "\n",
      "8. Model Selection Criteria\n",
      "A model is acceptable only if:\n",
      "- It outperforms baselines meaningfully\n",
      "- It respects leakage and grouping constraints\n",
      "- Its error behavior is stable across configurations\n",
      "\n",
      "Trade-off awareness:\n",
      "- Slightly worse RMSE may be accepted for:\n",
      "  - higher robustness\n",
      "  - better interpretability\n",
      "  - lower operational risk\n",
      "\n",
      "=== Deliverable 3 Framing Complete ===\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deliverable 3 — ML Framing & Experimentation Constraints\n",
    "========================================================\n",
    "\n",
    "THEORY\n",
    "------\n",
    "This script defines the *normative decision framework*\n",
    "for model experimentation.\n",
    "\n",
    "It translates findings from Deliverable 2 into:\n",
    "- data handling policies\n",
    "- modeling constraints\n",
    "- evaluation metrics\n",
    "- governance rules\n",
    "\n",
    "NO model is trained here.\n",
    "NO performance is measured.\n",
    "\n",
    "This file defines the \"rules of the game\" for Deliverable 4.\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "- train.csv (raw, unchanged dataset)\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "- Printed ML framing decisions\n",
    "- Explicit constraints for:\n",
    "  - data handling\n",
    "  - model eligibility\n",
    "  - evaluation metrics\n",
    "  - validation logic\n",
    "\n",
    "TERMINAL USAGE\n",
    "--------------\n",
    "python ml_framing.py\n",
    "\n",
    "EPISTEMIC SCOPE\n",
    "---------------\n",
    "Allowed:\n",
    "- Normative decisions\n",
    "- Policy definitions\n",
    "- Metric justification\n",
    "- Risk-based constraints\n",
    "\n",
    "Forbidden:\n",
    "- Model fitting\n",
    "- Cross-validation\n",
    "- Any numerical performance evaluation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    print(\"\\n=== Deliverable 3: ML Framing & Metrics Justification ===\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Problem Framing\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"1. Problem Type\")\n",
    "    print(\"- Supervised regression\")\n",
    "    print(\"- Continuous target: test bench duration (y)\")\n",
    "    print(\"- Static, tabular dataset\")\n",
    "    print(\"- No temporal forecasting\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Data Handling Policies (Derived from Deliverable 2)\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"2. Data Handling Policies\")\n",
    "\n",
    "    print(\"\\n2.1 Missingness Policy\")\n",
    "    print(\"- No NaN values observed\")\n",
    "    print(\"- Zeros represent structural absence of options\")\n",
    "    print(\"- No imputation will be applied\")\n",
    "    print(\"- Models must handle sparsity explicitly\")\n",
    "\n",
    "    print(\"\\n2.2 Outlier Policy\")\n",
    "    print(\"- Extreme test durations are real operational cases\")\n",
    "    print(\"- No row-level removal or clipping\")\n",
    "    print(\"- Robustness handled via loss functions or regularization\")\n",
    "\n",
    "    print(\"\\n2.3 Rare Configuration Policy\")\n",
    "    print(\"- Rare configurations are retained\")\n",
    "    print(\"- No grouping or collapsing at data level\")\n",
    "    print(\"- Generalization risk acknowledged and monitored\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Leakage Prevention Strategy\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n3. Leakage Prevention Strategy\")\n",
    "\n",
    "    print(\"- ID column must never be used as a feature\")\n",
    "    print(\"- Repeated configurations exist (up to 9 occurrences)\")\n",
    "    print(\"- Train/validation splits must be configuration-aware\")\n",
    "    print(\"- Identical configurations must not leak across splits\")\n",
    "\n",
    "    print(\"Approved validation strategies:\")\n",
    "    print(\"- Group-based split using full configuration signature\")\n",
    "    print(\"- OR time-aware split if temporal order becomes available\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. Feature Policy\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n4. Feature Policy\")\n",
    "\n",
    "    print(\"- Nominal features (X0–X8):\")\n",
    "    print(\"  - Treated as categorical\")\n",
    "    print(\"  - Encoding strategy must preserve interpretability\")\n",
    "\n",
    "    print(\"- Binary features (X10+):\")\n",
    "    print(\"  - Sparse, high-dimensional\")\n",
    "    print(\"  - No feature removal without justification\")\n",
    "    print(\"  - Regularization preferred over hard selection\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. Model Eligibility Constraints\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n5. Model Eligibility Constraints\")\n",
    "\n",
    "    print(\"Eligible model families must:\")\n",
    "    print(\"- Support regression\")\n",
    "    print(\"- Handle high-dimensional sparse inputs\")\n",
    "    print(\"- Be stable under rare configurations\")\n",
    "\n",
    "    print(\"\\nInterpretability constraint:\")\n",
    "    print(\"- Model behavior must be explainable at feature-group level\")\n",
    "    print(\"- Pure black-box models require additional justification\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6. Evaluation Metrics (Normative, not measured here)\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n6. Evaluation Metrics\")\n",
    "\n",
    "    print(\"Primary metric:\")\n",
    "    print(\"- RMSE (penalizes large scheduling errors)\")\n",
    "\n",
    "    print(\"\\nSecondary metrics:\")\n",
    "    print(\"- MAE (robust to outliers)\")\n",
    "    print(\"- Error distribution diagnostics\")\n",
    "\n",
    "    print(\"\\nMetric usage constraints:\")\n",
    "    print(\"- Metrics must be reported with confidence intervals\")\n",
    "    print(\"- Single-point estimates are insufficient\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 7. Baseline Definition\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n7. Baseline Definition\")\n",
    "\n",
    "    print(\"Mandatory baselines for Deliverable 4:\")\n",
    "    print(\"- Global mean predictor\")\n",
    "    print(\"- Simple linear regression\")\n",
    "\n",
    "    print(\"Purpose of baselines:\")\n",
    "    print(\"- Sanity check\")\n",
    "    print(\"- Value justification of complex models\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 8. Decision Criteria for Model Selection\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\n8. Model Selection Criteria\")\n",
    "\n",
    "    print(\"A model is acceptable only if:\")\n",
    "    print(\"- It outperforms baselines meaningfully\")\n",
    "    print(\"- It respects leakage and grouping constraints\")\n",
    "    print(\"- Its error behavior is stable across configurations\")\n",
    "\n",
    "    print(\"\\nTrade-off awareness:\")\n",
    "    print(\"- Slightly worse RMSE may be accepted for:\")\n",
    "    print(\"  - higher robustness\")\n",
    "    print(\"  - better interpretability\")\n",
    "    print(\"  - lower operational risk\")\n",
    "\n",
    "    print(\"\\n=== Deliverable 3 Framing Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6f81ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw dataset...\n",
      "Saving cleaned dataset...\n",
      "Writing cleaning report...\n",
      "\n",
      "=== Cleaning Complete ===\n",
      "Original dataset shape: (4209, 378)\n",
      "Removed column: ID (governance / leakage risk)\n",
      "Categorical features retained: 8\n",
      "Binary features before cleaning: 368\n",
      "Removed 12 inactive binary features (always 0)\n",
      "Active binary features retained: 356\n",
      "Target variable y preserved without modification\n",
      "Final dataset shape: (4209, 365)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deliverable 3 → 4 Bridge\n",
    "========================\n",
    "Data Cleaning & Framing Operationalization\n",
    "\n",
    "This script applies ONLY the normative decisions\n",
    "defined in Deliverable 3.\n",
    "\n",
    "It does NOT:\n",
    "- train models\n",
    "- evaluate performance\n",
    "- change target semantics\n",
    "\n",
    "It DOES:\n",
    "- enforce governance constraints\n",
    "- remove non-informative features\n",
    "- prepare a clean, model-ready dataset\n",
    "\n",
    "INPUT\n",
    "-----\n",
    "../data/train.csv  (raw, unchanged)\n",
    "\n",
    "OUTPUT\n",
    "------\n",
    "../data/train_clean.csv\n",
    "../data/cleaning_report.txt\n",
    "\n",
    "TERMINAL USAGE\n",
    "--------------\n",
    "python clean_data.py\n",
    "\n",
    "EPISTEMIC STATUS\n",
    "----------------\n",
    "Normative → Operational\n",
    "(No empirical validation yet)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "RAW_PATH = \"../data/train.csv\"\n",
    "OUT_PATH = \"../data/train_clean.csv\"\n",
    "REPORT_PATH = \"../data/cleaning_report.txt\"\n",
    "\n",
    "def main():\n",
    "    report = []\n",
    "\n",
    "    print(\"Loading raw dataset...\")\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    original_shape = df.shape\n",
    "\n",
    "    report.append(f\"Original dataset shape: {original_shape}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Remove governance-only identifiers\n",
    "    # --------------------------------------------------\n",
    "    if \"ID\" in df.columns:\n",
    "        df = df.drop(columns=[\"ID\"])\n",
    "        report.append(\"Removed column: ID (governance / leakage risk)\")\n",
    "    else:\n",
    "        report.append(\"No ID column found\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Identify feature groups\n",
    "    # --------------------------------------------------\n",
    "    feature_cols = [c for c in df.columns if c.startswith(\"X\")]\n",
    "    categorical = [c for c in feature_cols if df[c].dtype == object]\n",
    "    binary = [c for c in feature_cols if df[c].dropna().isin([0, 1]).all()]\n",
    "\n",
    "    report.append(f\"Categorical features retained: {len(categorical)}\")\n",
    "    report.append(f\"Binary features before cleaning: {len(binary)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Remove inactive one-hot features (always 0)\n",
    "    # --------------------------------------------------\n",
    "    inactive_binary = [c for c in binary if df[c].mean() == 0]\n",
    "\n",
    "    if inactive_binary:\n",
    "        df = df.drop(columns=inactive_binary)\n",
    "        report.append(\n",
    "            f\"Removed {len(inactive_binary)} inactive binary features (always 0)\"\n",
    "        )\n",
    "    else:\n",
    "        report.append(\"No inactive binary features found\")\n",
    "\n",
    "    active_binary = len(binary) - len(inactive_binary)\n",
    "    report.append(f\"Active binary features retained: {active_binary}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Target integrity check\n",
    "    # --------------------------------------------------\n",
    "    if \"y\" not in df.columns:\n",
    "        raise ValueError(\"Target column 'y' missing after cleaning!\")\n",
    "\n",
    "    if df[\"y\"].isna().any():\n",
    "        raise ValueError(\"Unexpected NaNs in target variable!\")\n",
    "\n",
    "    report.append(\"Target variable y preserved without modification\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Final consistency checks\n",
    "    # --------------------------------------------------\n",
    "    final_shape = df.shape\n",
    "    report.append(f\"Final dataset shape: {final_shape}\")\n",
    "\n",
    "    print(\"Saving cleaned dataset...\")\n",
    "    df.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "    print(\"Writing cleaning report...\")\n",
    "    with open(REPORT_PATH, \"w\") as f:\n",
    "        for line in report:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    print(\"\\n=== Cleaning Complete ===\")\n",
    "    for line in report:\n",
    "        print(line)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
