{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df41480",
   "metadata": {},
   "source": [
    "# 5. KPI–OKR Mapping & Implementation\n",
    "*Aligned with Project Deliverables: KPI–OKR Mapping, Supporting KPIs, and ML-to-Product Metric Mapping*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11b826",
   "metadata": {},
   "source": [
    "### 5.1 Strategic Framework: Greener Manufacturing\n",
    "To ensure our ML project delivers real business value, we map our technical goals directly to the **Mercedes-Benz Ambition 2039** strategy. We are not just predicting numbers; we are optimizing physical factory time to reduce energy usage.\n",
    "\n",
    "#### **The Objective (O): Optimize Vehicle Testing for Sustainability**\n",
    "We aim to reduce the time cars spend on test benches, which directly lowers energy consumption (electricity/fuel) and increases factory throughput.\n",
    "\n",
    "#### **Key Results (KR) & Connected KPIs**\n",
    "We define success through three distinct layers:\n",
    "\n",
    "| Key Result (KR) | Supporting KPI (Metric) | Goal | Why it matters |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **KR 1:** Increase Test Bench Efficiency by 10% | **Mean Absolute Error (MAE)** | Minimize | Shorter tests mean less energy per unit. We need to know (in seconds) how close our prediction is to reality. |\n",
    "| **KR 2:** Maintain 98% Logistics Reliability | **Under-prediction Rate** | $< 5\\%$ | **Critical Risk:** If we predict a test is *short* but it takes *long*, the line stops. This measures the % of dangerous \"optimistic\" errors. |\n",
    "| **KR 3:** Validated ML Performance | **$R^2$ Score** | $> 0.55$ | The model must be statistically valid (better than baseline) to be deployed. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c563f5",
   "metadata": {},
   "source": [
    "### 5.3 Implementation: The Executive Dashboard\n",
    "The following code implements this mapping. It transforms raw model predictions into a color-coded dashboard that a Product Manager can read instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cc8af9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_ebm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m styled_df\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# --- EXECUTION BLOCK ---\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# 1. Prepare your dictionary of predictions\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Replace `y_pred_ebm`, `y_pred_gbr`, `y_pred_huber` with your actual variables\u001b[39;00m\n\u001b[32m     85\u001b[39m model_predictions = {\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEBM (Explainable)\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43my_pred_ebm\u001b[49m,    \u001b[38;5;66;03m# Replace with your EBM predictions\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGBR (Boosting)\u001b[39m\u001b[33m\"\u001b[39m: y_pred_gbr,       \u001b[38;5;66;03m# Replace with your GBR predictions\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHuber (Robust)\u001b[39m\u001b[33m\"\u001b[39m: y_pred_huber      \u001b[38;5;66;03m# Replace with your Huber predictions\u001b[39;00m\n\u001b[32m     89\u001b[39m }\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 2. Run the comparison\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Ensure `y_val` is your ground truth array\u001b[39;00m\n\u001b[32m     93\u001b[39m comparison_dashboard = compare_model_kpis(y_val, model_predictions)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_pred_ebm' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def compare_model_kpis(y_true, predictions_dict):\n",
    "    \"\"\"\n",
    "    Generates a comparative KPI dashboard for multiple models (EBM, GBR, Huber).\n",
    "    \n",
    "    Args:\n",
    "    y_true (array-like): Ground truth values.\n",
    "    predictions_dict (dict): Dictionary of model names and their prediction arrays.\n",
    "                             e.g., {'EBM': y_pred_ebm, 'GBR': y_pred_gbr...}\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Constants for Business Impact\n",
    "    CO2_FACTOR_PER_SEC = 0.05  # Hypothetical grams of CO2 per second\n",
    "    RISK_THRESHOLD = 0.05      # Max 5% under-prediction allowed\n",
    "\n",
    "    for model_name, y_pred in predictions_dict.items():\n",
    "        # --- 1. Technical Metric (KR3) ---\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # --- 2. Operational Metric (KR1) ---\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        # --- 3. Risk Metric (KR2 - The \"Safety\" Check) ---\n",
    "        # Residual < 0 means we predicted LESS time than it actually took (Blockage Risk)\n",
    "        residuals = y_pred - y_true\n",
    "        under_prediction_count = np.sum(residuals < 0)\n",
    "        under_prediction_rate = under_prediction_count / len(y_true)\n",
    "        \n",
    "        # --- 4. Business Impact ---\n",
    "        total_error_seconds = np.sum(np.abs(residuals))\n",
    "        wasted_emissions = total_error_seconds * CO2_FACTOR_PER_SEC\n",
    "        \n",
    "        # Determine Status based on our hierarchy of needs:\n",
    "        # 1. MUST PASS Risk Check. 2. MUST PASS R2 Threshold.\n",
    "        status = \"TRACK\"\n",
    "        if under_prediction_rate > RISK_THRESHOLD:\n",
    "            status = \"CRITICAL FAIL\" # Risk is too high, regardless of R2\n",
    "        elif r2 < 0.55:\n",
    "            status = \"FAIL (Low Signal)\"\n",
    "        else:\n",
    "            status = \"PASS\"\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"R2 Score (Tech)\": r2,\n",
    "            \"MAE [s] (Efficiency)\": mae,\n",
    "            \"Blockage Risk %\": under_prediction_rate,\n",
    "            \"Est. Wasted CO2 [g]\": wasted_emissions,\n",
    "            \"Status\": status\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # --- Styling ---\n",
    "    # Highlight the \"Winner\" (Pass status) and \"Losers\" (Critical Fail)\n",
    "    def highlight_rows(row):\n",
    "        if \"CRITICAL\" in row['Status']:\n",
    "            return ['background-color: #FFE4E1; color: black'] * len(row) # Light Red\n",
    "        elif row['Status'] == \"PASS\":\n",
    "            return ['background-color: #F0FFF0; color: black'] * len(row) # Honeydew Green\n",
    "        else:\n",
    "            return [''] * len(row)\n",
    "\n",
    "    # Format numbers\n",
    "    styled_df = df.style.apply(highlight_rows, axis=1)\\\n",
    "        .format({\n",
    "            \"R2 Score (Tech)\": \"{:.4f}\",\n",
    "            \"MAE [s] (Efficiency)\": \"{:.2f}\",\n",
    "            \"Blockage Risk %\": \"{:.1%}\",\n",
    "            \"Est. Wasted CO2 [g]\": \"{:.2f}\"\n",
    "        })\\\n",
    "        .set_caption(\"Model Selection Dashboard: Greener Manufacturing\")\\\n",
    "        .hide_index()\n",
    "        \n",
    "    return styled_df\n",
    "\n",
    "# --- EXECUTION BLOCK ---\n",
    "# 1. Prepare your dictionary of predictions\n",
    "# Replace `y_pred_ebm`, `y_pred_gbr`, `y_pred_huber` with your actual variables\n",
    "model_predictions = {\n",
    "    \"EBM (Explainable)\": y_pred_ebm,    # Replace with your EBM predictions\n",
    "    \"GBR (Boosting)\": y_pred_gbr,       # Replace with your GBR predictions\n",
    "    \"Huber (Robust)\": y_pred_huber      # Replace with your Huber predictions\n",
    "}\n",
    "\n",
    "# 2. Run the comparison\n",
    "# Ensure `y_val` is your ground truth array\n",
    "comparison_dashboard = compare_model_kpis(y_val, model_predictions)\n",
    "display(comparison_dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717190fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
